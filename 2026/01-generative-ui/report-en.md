** NOTE: This is a machine-translated document and is provided for reference only. In case of any discrepancies, the [Chinese version](/report-zh.md) shall prevail. **

# Generative UI Special Session Summary

- [Overview](#overview)
- [Opening Discussion (Chunming Hu, Zuo Wang)](#opening-discussion-chunming-hu-zuo-wang)
- [Industry Practice](#industry-practice)
  - [Chen Lu (Ant Group)](#1-chen-lu-ant-group)
  - [Chunhui Mo (Huawei)](#2-chunhui-mo-huawei)
  - [Ming Zu (Baidu)](#3-ming-zu-baidu)
- [Round Table Discussions](#round-table-discussions)
  - [Q1: Performance & Loading](#q1-in-ai-chat-scenarios-does-UI-load-performance-still-matter)
  - [Q2: Standard Collaboration & Ecosystem](#q2-how-to-optimize-platforms-and-ecosystems-through-standard-collaboration)
  - [Q3: UI Components](#q3-ui-components)
  - [Q4: Web Subset](#q4-web-subset)
  - [Q5: Testing & Verification](#q5-testing-and-verification)
- [Conclusions & Next Steps](#conclusions--next-steps)


## Overview

With the rapid development of large language models and generative AI, Generative UI has become a highly visible area of interest across the industry. While multiple technical approaches are being explored, practitioners are also facing a new set of challenges around performance and efficiency, security risks, scalability, and interoperability.

To further discuss the development trends of Generative UI on the Web and its standardization potential, W3C China organized an online meeting on 29 January 2026, which was attended by over 50 participants from W3C member organizations.

---

## Opening Discussion (Chunming Hu, Zuo Wang)

The meeting opened with remarks and perspectives from **Prof. Chunming Hu, W3C Advisory Board member from Beihang University**, and **Dr. Zuo Wang from Huawei Fields Laboratory**.

**Prof. Chunming Hu** observed that in the Web domain, the focus of AI-assisted software development is gradually shifting toward frontend UI. As large models and agents become part of human‚Äìcomputer interaction, the UI is emerging as a new critical interaction layer. At present, MCP/agents widely adopt Generative UI, expressing interaction requirements via **Markdown, HTML, or other DSLs**.

Around the future expression of this interaction layer and its standardization space, he raised several questions worthy of deeper discussion:

* Should standards **extend capabilities on top of Markdown**, or **appropriately simplify HTML**?
* How can generative UI achieve a **balance between generation and constraints** with AI assistance?
* How do ongoing W3C explorations around **high-performance HTML/CSS subsets, mini-program forms, and "meta-services"** relate to these topics?

He also mentioned that the **W3C Advisory Committee Meeting ([AC 2026](https://www.w3.org/zh-hans/events/))** will be held in **Hangzhou, China**, and that forming consensus ahead of that meeting would help enable deeper exchanges and discussions.

---

**Dr. Zuo Wang** shared observations and thoughts on **"Deep UI"** from the perspective of how the Web entry point and application form are changing in the era of large models:

* **On PC**: users still mainly interact with models through the browser, with models controlling pages via the Web engine and MCP.
* **On Mobile**: app and service distribution is increasingly mediated by models, and the Web pages that used to live inside traditional apps or MiniApps are migrating into **model-generated UIs**.

Such UIs offer higher production efficiency and stronger real-time personalization, and are expected to become an important interface for future human‚Äìcomputer interaction.

![Illustration](img-1.png)

Regarding the technical evolution of generative UI, he believes the industry has progressed from Markdown as the carrier, to template-based UI, and further towards more fine-grained, model-directly-generated interface expressions. Currently, major models and platforms often define their own proprietary UI technology stacks, which brings high costs for cross-platform adaptation and service integration. Based on this, he proposed exploring whether there exists a model-friendly, cross-platform universal standard language to reduce redundant adaptations, suggesting this direction could be included in relevant standardization discussions at W3C.

![Illustration](img-2.png)

---

**Ruoxi Ran**, W3C team member and contact person for the [Web & AI Interest Group](https://www.w3.org/groups/ig/webai/), moderated the meeting discussions. He noted that with the development of large models and agent technologies, UI is shifting from static design and template composition to on-demand generation and dynamic adaptation. Generative UI is often autonomously generated by models, which poses new questions and opportunities for Web architecture, interaction methods, and standard systems. W3C hopes that through this exchange, it can understand real industry practices and explore whether and how related technologies can be incorporated into future Web standard discussions.

---

## Industry Practice

Several technical experts from the industry shared their respective understandings and hands-on experience with generative UI.

### 1. Chen Lu (Ant Group)

üìÑ Slides:
[https://www.w3.org/2026/01/GenUI-China/Perspectives_of_Generative_UI_RH.pptx](https://www.w3.org/2026/01/GenUI-China/Perspectives_of_Generative_UI_RH.pptx)

Drawing on experience from Ant and the broader industry, Lu shared his observations and thinking on Generative UI:

* As large models‚Äô coding capabilities and coding agents advance rapidly, confidence in using models to generate Web pages and applications has grown significantly, and Generative UI is becoming a major focus.
* The route of having models generate full HTML increasingly overlaps with the scope of AI coding, prompting a rethinking of how Generative UI should be defined and what its key discussion topics should be.

He analyzed three technical routes: Markdown, DSLs, and direct HTML output, and noted that interest in HTML-direct and DSL-based approaches is clearly rising. Direct HTML offers a very high upper bound in expressive power, but comes with substantial challenges in security, performance, and infrastructure cost. By contrast, DSL-based approaches leverage componentization and controlled rendering to gain advantages in security, performance, and cross-platform consistency, but because each vendor can cheaply build its own DSL, the ecosystem is highly fragmented.

Based on this, Chen Lu suggested standardization efforts should focus on three areas:

* Explore a model-friendly, lightweight Web subset to improve security.
* Study better sandboxing and isolation mechanisms.
* Promote a cross-vendor, common intermediate protocol for Generative UI to lay the foundation for future ecosystem collaboration.

---

### 2. Chunhui Mo (Huawei)

üìÑ Slides:
[https://www.w3.org/2026/01/GenUI-China/Generative_UI_Technology_Insights_MCH.pptx](https://www.w3.org/2026/01/GenUI-China/Generative_UI_Technology_Insights_MCH.pptx)

Mo provided a systematic analysis of the technical essence and deployment path of Generative UI from an engineering perspective. He argued that the core of Generative UI is not AI assisting template selection, but its real-time generation capability: at runtime, the model must dynamically generate page structure, component types, layout, and interaction logic based on user intent and context. Most current practices still operate at a design-time preset-UI stage, essentially functioning as template orchestration systems where AI only participates in parameter filling and decision support; coverage remains constrained by template scope and struggles to serve long-tail scenarios.

From the perspective of enterprise practice, he proposed ‚Äúscenario coverage‚Äù as a key metric for generation capability: template-based approaches are fundamentally limited by template inventory, whereas in principle, Generative UI should be able to adapt across the full scenario space. In terms of technical evolution, he suggested that Generative UI will move from a template phase, through a phase of dynamically assembling atomic components, and eventually toward full generation, which imposes higher requirements on the runtime environment and standardization.

On the implementation side, he compared the suitability of JSON and XML as DSLs, noting that XML is more favorable for frontend parsing and real-time rendering under streaming output and incomplete structures. He also emphasized the need for a unified rendering architecture to support consistent behavior across Web and native platforms, and for safeguards to keep generated output under control. To address fragmented experience across multiple generated pages, he proposed the idea of ‚Äúgenerative MiniApps,‚Äù where AI orchestrates multi-page flows to support complex business end-to-end. He stressed that Generative UI must support integration of enterprise-specific components and mixing text, structured UI, and DSL output to be truly deployable.

He proposed the following next steps:
* Explore capability requirements at the browser-engine level.
* Promote standardization related to generative UI.
* Define security guardrail specifications.
* Build more general DSL specifications.
* Research unified multi-end rendering frameworks.

---

### 3. Ming Zu (Baidu)

Ming introduced Baidu‚Äôs ongoing work on Generative UI, which overall follows a broadly similar line of thinking to the previous speakers. Since beginning to explore AI applications in early 2023, Baidu has continuously extended Markdown to support richer and more powerful components in conversational product paradigms. In parallel, as part of its agent ecosystem, Baidu has defined a JSON-based DSL to provide layout and interaction capabilities while reducing integration costs for developers. Through several years of iteration, this has grown into a relatively mature solution.

At the end of last year, they systematically integrated the technical accumulations from the past few years, including core capabilities such as protocols, rendering engines, and component libraries, and open-sourced it externally üëâ [https://github.com/baidu/cosui/](https://github.com/baidu/cosui/)

Recent work focuses on using large models to generate code directly, particularly to deliver richer interactive applications in search scenarios. In this process, Baidu has encountered many of the same issues raised by other speakers and hopes to continue exchanging with the wider community to jointly advance and land related technologies.

---
## Round Table Discussions

### Q1: In AI chat scenarios, does UI load performance still matter?

**Zuo Wang:**
In the evolution of mobile technologies, the ecosystem has moved from purely native to Web, and then to cross-platform stacks, with cross-platform approaches aiming to stay close to native performance and load times while preserving cross-platform advantages. Cross-platform technologies emerged largely because users were dissatisfied with ‚Äúslow-loading‚Äù Web pages.

However, in current AI conversational and generative application scenarios, user mental models have shifted. In traditional page scenarios, users expect ‚Äúclick and instant response‚Äù and are highly sensitive to latency, whereas in AI chat scenarios, users are more inclined to treat the system as a ‚Äúthinking agent,‚Äù and therefore show greater tolerance and patience toward response time.

In Generative UI scenarios where the interface is dynamically loaded and generated via Web technologies, the question arises: does load speed still impact user experience as critically as in traditional applications? Put differently, will users lower their expectations for instantaneous performance because they anticipate that ‚Äúthe AI needs to think‚Äù? He invited perspectives from different vendors on how they position this issue in product design.

**Ming Zu:**
In Baidu‚Äôs search scenarios, performance requirements remain relatively strict, which is a major practical constraint and challenge.

**Ruoxi Ran:**
Ruoxi briefly complemented the performance discussion, noting that related work will continue and be shared at appropriate times. He referenced Mo‚Äôs earlier notion of a current ‚Äúimpossible triangle‚Äù in Generative UI, which prompted further reflection. Across different access modalities‚ÄîAPI, local deployment, or Web‚Äîusers today generally accept that large models take time to generate content and are relatively tolerant of latency

In the longer term, however, he is unsure this tolerance will persist. Historically, users were once willing to wait a minute for Web pages to load, but expectations have tightened as technology and experience improved. A similar trend may emerge as large models and AI applications become ubiquitous, with expectations of performance rising and tolerance of latency decreasing.

At the current stage, we likely must confront a similar ‚Äúimpossible triangle‚Äù trade-off between performance, flexibility, and experience, and search for a reasonable balance while continually optimizing performance. He suggested that this is a topic worthy of deeper discussion both within W3C and across the broader industry.

**Chunhui Mo:**
Mo approached the topic from the standpoint of performance metrics and the relationship between Generative UI and large models. In LLMs, TTFT (Time To First Token) and TPOT (Time Per Output Token) are key performance indicators; he wondered whether Generative UI should develop analogous metrics to evaluate user-perceived performance.

One possible approach is to treat mature LLM metrics as inputs when designing Generative UI metrics. For example, even before the final result is produced, early feedback‚Äîsuch as chain-of-thought snippets or partial content‚Äîis already a meaningful form of response. The critical factor becomes the wait time between submission and the first useful feedback, and how tolerant users are of that time.

In static UI design, the ‚Äúthree-second rule‚Äù has often been applied, beyond which user drop-off increases sharply; this clearly no longer fits Generative UI. Since LLM computation inherently takes time, user expectations and tolerance are higher, but entirely blank screens with one or two minutes of no feedback remain unacceptable.

He suggested that the community should focus on how to define a performance metric system tailored to Generative UI. For example, following the TTFT paradigm, the ‚Äútime to first visible feedback‚Äù could be a core metric. For UI, more fine-grained indicators are also needed, such as:
- Time until the first UI element appears;
- The cadence of subsequent rendering;
- Time intervals between each chunk of text or UI component.

In essence, the progressive display of text or UI elements is analogous to token streaming, and the timing of each visible unit could serve as an important dimension in Generative UI performance evaluation. He recommended building a quantitative performance metric system inspired by TTFT and token streaming to assess ‚Äútime to first view‚Äù and the efficiency of progressive UI presentation.

---

### Q2: How to optimize platforms and ecosystems through standard collaboration

**Zuo Wang:**
Wang returned to the issue of ‚Äúproprietary standards,‚Äù which several speakers had also raised. From his perspective, both service providers and platform vendors (including LLM and digital environment providers) are currently defining their own UI description formats based on their specific needs. This leads him to ask whether we need‚Äîand whether it is feasible‚Äîto move toward unification of Generative UI-related standards.

As LLMs evolve into service distribution platforms, they will ultimately need to present generated interactive interfaces to users, often produced collaboratively by multiple services or agents. If each platform defines its own private UI description, developers and service providers may have to integrate with many different protocols and parsers, significantly inflating integration costs and harming ecosystem health.

He argued that at the level of UI description‚Äîwhether via HTML/CSS or more abstract DSLs‚Äîwe should seek consensus to push this layer toward standardization and unification. If different models and platforms can converge on relatively consistent description formats, it will benefit developers, service providers, and the ecosystem, and better support the long-term development of Generative UI.

**Chunming Hu:**
Hu added two points. First, he believes there will ultimately need to be a standard at the UI presentation layer. Regardless of output format, generated content must be rendered by some runtime environment; even Markdown requires a renderer. More complex interactions require richer APIs and ultimately a View-like runtime layer for display and interaction.

Architecturally, there is therefore an inherent layering:
- A generation layer where models produce some constrained DSL;
- A runtime layer, from lightweight to more complex, that parses the DSL and implements interaction.

This ‚Äúgeneration + runtime‚Äù decomposition exists objectively, independent of personal preference.
‚Äã
Second, regarding cross-vendor unification, while vendors could each define their own DSLs and runtimes, this would result in duplicated effort and high maintenance costs. If we standardize this layer, or at least define capability tiers and ‚Äúsubset profiles,‚Äù content production and runtime environments can be decoupled, which is beneficial at an industry level.
‚Äã
He also noted a concrete demand: in scenarios where multiple models and service providers jointly generate a UI, brand owners (for example, airlines or travel services) still expect coherent brand-consistent experiences. Introducing template-like intermediate mechanisms so that Generative UI fills content into predefined templates might better address such requirements. If such needs are common, then defining cross-vendor norms at this intermediate layer could carry both technical and business value.

---

### Q3: UI Components

**Chunhui Mo:**
Mo highlighted component convergence as a central issue for Generative UI. Because UI inherently involves components, and current component libraries vary significantly across vendors and scenarios, it is unrealistic to expect everyone to adopt a single shared library.
‚Äã

If we aim to build a rendering engine for Generative UI that renders model outputs at runtime, we must confront the question of what component system it should be based on. Each vendor and business domain has its own components, making it difficult to enforce convergence at the framework or library level. Under these conditions, he argued that the only direction with a realistic chance of convergence for a standardized runtime is Web technologies‚Äîthe core area of W3C.
‚Äã

Within this framing, his team is exploring approaches that allow models to output DSLs describing component usage while supporting various stacks (React, Vue, Web Components, etc.). Before rendering, components from different stacks are normalized into standard Web components. He cited Google‚Äôs A2UI protocol as a reference, which also attempts to converge underlying representation by supporting Angular and Web Components and planning to support more frameworks. In their own work, once components are converted into Web Components, the browser-based rendering engine only needs to handle standard Web components without caring about the original stack.
‚Äã

He cautioned that DSLs which directly encode specific component libraries are unlikely to become general standards. A more robust approach is to follow A2UI-like patterns where the standard describes a mechanism for declaring available components and their parameter capabilities to the model. The model then generates UIs based on those declared capabilities, without depending on any particular implementation. He believes this layer can achieve meaningful convergence and is a promising focus for future work.

---

### Q4: Web Subset

**Xiaoqian Wu:**
Introduced the [joint discussion](https://www.w3.org/community/high-perf-baseline/2025/12/17/tpac-2025/) of the W3C MiniApps Working Group, the W3C High-Performance Web Applications Community Group, the WebView Community Group, the IWA Community, and the PWA Widget Community.

**Fuqiao Xue:**
Xue noted that, based on GitHub discussions and the meeting talks, Generative UI seems to be advancing along two primary paths:
- Direct generation of Web pages (HTML/CSS/JS);
- Generation of constrained card-style UIs based on component libraries, defined in JS, XML, etc. and rendered via component systems.

He believes these two paths are not mutually exclusive and can be considered simultaneously.

For the direction of directly generating Web, its flexibility and upper limit are very high, capable of generating rich interfaces, but it also brings some issues, especially security and communication issues. To balance flexibility and security, the scheme I previously considered is:
- Establish a restricted subset of HTML/CSS/JS, accompanied by rendering guidelines;
- Similar to the runtime environment of mini-programs, stipulate allowed elements and attributes while excluding high-risk or high-resource-consumption features, such as prohibiting the use of `<script>` tags and prohibiting loading excessive external resources;
- Ensure security through sandboxing rules.

On the other hand, we can also consider developing standardized DSLs:
- Enable interfaces generated by models or service providers to be reliably rendered in different environments;
- When training new models, the model can learn DSL grammar to produce interfaces more efficiently;
- The specific syntax and design of the DSL need to be continuously explored and adjusted in practice.

Additionally, regarding the component library issue, W3C's [Open UI Community Group](https://www.w3.org/groups/cg/open-ui/) is currently working on UI component standardization. We can pay attention and refer to their ideas.

In general, he believes in the future we can explore both paths‚Äîdirect Web and DSL standardization‚Äîsimultaneously to provide feasible solutions for the security, controllability, and cross-platform rendering of generative UI.

---

### Q5: Testing and Verification

**Zuo Wang:**
On testing and verification, Wang observed that while traditional Web development relies on human testing before release, this becomes infeasible for model-generated UIs, especially complex ones. He argued for dedicated testing processes and infrastructure, potentially with virtualized environments where model-generated UIs are first validated for correctness and reliability. Some practitioners already use models to automatically generate and run test cases on the server or in virtual environments to verify outputs before delivering them to users. In summary, he stressed that Generative UI reliability must rely more on automated testing and virtualization than on manual testing, in order to ensure that users see trustworthy content.

**Ruoxi Ran:**
Regarding the accessibility and testing of generative UI, Ruoxi emphasized the following topics:
- UI is highly related to accessibility. If the content of generative UI does not comply with accessibility standards, it may bring compliance risks, such as facing product removal in Europe and the US, etc.;
- Existing automated accessibility testing tools do not fully cover all standards, leaving gaps for Generative UI;
- W3C can learn from the experience of the WPT ([Web Platform Tests](https://github.com/web-platform-tests/wpt)) platform project to develop an open testing platform or tooling for automated verification of Generative UI, including accessibility;
- The goal would be to provide a shared open platform for testing accessibility and other key quality metrics across vendors and developers.

**Fuqiao Xue:**
Fuqiao proposed that:
- W3C‚Äôs existing Browser Testing and Tools work, including the WebDriver protocol, could serve as a foundation for Generative UI testing and verification.
- Methods for automatically generating, running, and evaluating generated UI can be explored, for example:
  - Automatically taking screenshots or capturing the generated UI;
  - Comparing the generated UI with user intent for verification;
  - If verification fails, the system can automatically attempt repairs until it meets the verification standards.

These methods can provide quantifiable testing means for the reliability and user experience of generative UI and can also serve as the basis for subsequent standardization and tooling.

---

## Conclusions & Next Steps

Following the meeting discussion, the Generative UI Community Group was established and plans to explore the following areas:

- **Evaluation & Performance:** Explore how to characterize and assess latency, responsiveness, and output quality of Generative UI in the Web context, and identify gaps in current W3C standardization.
- **Verification & Testing:** Investigate validation and testing methodologies for Generative UI that go beyond traditional end-to-end testing, including structured test cases and reference implementations.
- **Intermediate Representation:** Explore the potential value of lightweight, cross-vendor intermediate representations or protocols for Generative UI to improve interoperability and model compatibility.
- **Alignment with Web Platform:** Analyze whether Generative UI systems would benefit from a lightweight subset of Web technologies, while remaining consistent with existing Web principles.

All interested parties are welcome to join the [Generative UI Community Group](https://www.w3.org/groups/cg/gen-ui/) and participate in the discussion.
