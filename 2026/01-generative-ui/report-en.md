** NOTE: This is a machine-translated document and is provided for reference only. In case of any discrepancies, the [Chinese version](/report-zh.md) shall prevail. **

# Generative UI Special Exchange Meeting Summary

- [Overview](#overview)
- [Opening Discussion (Chunming Hu, Zuo Wang)](#opening-discussion-chunming-hu-zuo-wang)
- [Industry Practice](#industry-practice)
  - [Chen Lu (Ant Group)](#1-chen-lu-ant-group)
  - [Chunhui Mo (Huawei)](#2-chunhui-mo-huawei)
  - [Ming Zu (Baidu)](#3-ming-zu-baidu)
- [Open Discussion Session](#open-discussion-session)
  - [Q1: Performance & Loading Speed](#q1-is-loading-speed-still-important-for-ui-interfaces-generated-by-large-models-in-ai-chat-scenarios)
  - [Q2: Standard Collaboration & Ecosystem](#q2-how-to-optimize-platforms-and-ecosystems-through-standard-collaboration)
  - [Q3: UI Components](#q3-ui-components)
  - [Q4: Web Subset](#q4-web-subset)
  - [Q5: Testing & Verification](#q5-testing-and-verification)
- [Conclusions & Next Steps](#conclusions--next-steps)


## Overview

With the rapid development of large language models and generative AI, Generative UI has become a highly visible area of interest across the industry. While multiple technical approaches are being explored, practitioners are also facing a new set of challenges around performance and efficiency, security risks, scalability, and interoperability.

To further discuss the development trends of Generative UI on the Web and its standardization potential, W3C China organized an online workshop on 29 January 2026, which was attended by over 50 participants from W3C member organizations.

---

## Opening Discussion (Chunming Hu, Zuo Wang)

The workshop opened with remarks and perspectives from **Prof. Chunming Hu, W3C Advisory Board member from Beihang University**, and **Dr. Zuo Wang from Huawei Fields Laboratory**.

**Prof. Chunming Hu** observed that in the Web domain, the focus of AI-assisted software development is gradually shifting toward frontend UI. As large models and agents become part of humanâ€“computer interaction, the UI is emerging as a new critical interaction layer. At present, MCP/agents widely adopt Generative UI, expressing interaction requirements via **Markdown, HTML, or other DSLs**.

Around the future expression of this interaction layer and its standardization space, he raised several questions worthy of deeper discussion:

* Should standards **extend capabilities on top of Markdown**, or **appropriately simplify HTML**?
* How can generative UI achieve a **balance between generation and constraints** with AI assistance?
* How do ongoing W3C explorations around **high-performance HTML/CSS subsets, mini-program forms, and "meta-services"** relate to these topics?

He also mentioned that the **W3C Advisory Committee Meeting ([AC 2026](https://www.w3.org/zh-hans/events/))** will be held in **Hangzhou, China**, and that forming consensus ahead of that meeting would help enable deeper exchanges and discussions.

---

**Dr. Zuo Wang** shared observations and thoughts on **"Deep UI"** from the perspective of how the Web entry point and application form are changing in the era of large models:

* **On PC**: users still mainly interact with models through the browser, with models controlling pages via the Web engine and MCP.
* **On Mobile**: app and service distribution is increasingly mediated by models, and the Web pages that used to live inside traditional apps or MiniApps are migrating into **model-generated UIs**.

Such UIs offer higher production efficiency and stronger real-time personalization, and are expected to become an important interface for future humanâ€“computer interaction.

![Illustration](img-1.png)

Regarding the technical evolution of generative UI, he believes the industry has progressed from Markdown as the carrier, to template-based UI, and further towards more fine-grained, model-directly-generated interface expressions. Currently, major models and platforms often define their own proprietary UI technology stacks, which brings high costs for cross-platform adaptation and service integration. Based on this, he proposed exploring whether there exists a model-friendly, cross-platform universal standard language to reduce redundant adaptations, suggesting this direction could be included in relevant standardization discussions at W3C.

![Illustration](img-2.png)

---

**Ruoxi Ran**, W3C team member and contact person for the [Web & AI Interest Group](https://www.w3.org/groups/ig/webai/), moderated the meeting discussions. He noted that with the development of large models and agent technologies, UI is shifting from static design and template composition to on-demand generation and dynamic adaptation. Generative UI is often autonomously generated by models, which poses new questions and opportunities for Web architecture, interaction methods, and standard systems. W3C hopes that through this exchange, it can understand real industry practices and explore whether and how related technologies can be incorporated into future Web standard discussions.

---

## Industry Practice

Several technical experts from the industry shared their respective understandings and hands-on experience with generative UI.

### 1. Chen Lu (Ant Group)

ðŸ“„ Slides:
[https://www.w3.org/2026/01/GenUI-China/Perspectives_of_Generative_UI_RH.pptx](https://www.w3.org/2026/01/GenUI-China/Perspectives_of_Generative_UI_RH.pptx)

Drawing on experience from Ant and the broader industry, Lu shared his observations and thinking on Generative UI:

* As large modelsâ€™ coding capabilities and coding agents advance rapidly, confidence in using models to generate Web pages and applications has grown significantly, and Generative UI is becoming a major focus.
* The route of having models generate full HTML increasingly overlaps with the scope of AI coding, prompting a rethinking of how Generative UI should be defined and what its key discussion topics should be.

He analyzed three technical routes: Markdown, DSLs, and direct HTML output, and noted that interest in HTML-direct and DSL-based approaches is clearly rising. Direct HTML offers a very high upper bound in expressive power, but comes with substantial challenges in security, performance, and infrastructure cost. By contrast, DSL-based approaches leverage componentization and controlled rendering to gain advantages in security, performance, and cross-platform consistency, but because each vendor can cheaply build its own DSL, the ecosystem is highly fragmented.

Based on this, Chen Lu suggested standardization efforts should focus on three areas:

* Explore a model-friendly, lightweight Web subset to improve security.
* Study better sandboxing and isolation mechanisms.
* Promote a cross-vendor, common intermediate protocol for Generative UI to lay the foundation for future ecosystem collaboration.

---

### 2. Chunhui Mo (Huawei)

ðŸ“„ PPT:
[https://www.w3.org/2026/01/GenUI-China/Generative_UI_Technology_Insights_MCH.pptx](https://www.w3.org/2026/01/GenUI-China/Generative_UI_Technology_Insights_MCH.pptx)

From an engineering perspective, he systematically analyzed the technical essence and implementation path of Generative UI. He pointed out that the core of Generative UI lies not in AI-driven template selection, but in "Real-time Generation" capability: models need to dynamically generate page structures, component types, layout methods, and interaction logic at runtime based on user intent and context. Most current practices still fall under design-time preset UI, essentially template orchestration systems. AI only participates in parameter filling and decision support, limiting scenario coverage and making it difficult to support long-tail demands.

Focusing on enterprise practice, he proposed "scenario coverage rate" as a key metric for measuring generation capability: the coverage of templated solutions is constrained by template scale, while true generative UI can theoretically achieve full-scenario adaptation. In technical evolution, generative UI will progress through the template stage, the dynamic assembly of atomic components stage, and finally move towards a fully generative stage, imposing higher requirements on runtime environments and standardization capabilities.

At the implementation level, he compared the adaptability of JSON and XML as DSLs, noting that XML is more conducive to frontend parsing and real-time rendering under streaming output and incomplete structures. He also emphasized the need for a unified rendering architecture to support Web and Native multi-platform consistency and ensure controllable generation results through security guardrail mechanisms. Addressing the experience fragmentation caused by multi-page generation, he proposed a "Generative Mini-Program" approach, using AI to unify the scheduling of multi-page logic and support complex business closed loops. He stressed that generative UI must support the integration of enterprise-owned components and allow mixed output of text, structured UI, and DSL to achieve real-world adoption.

He proposed the following next steps:
* Explore capability requirements at the browser kernel level.
* Promote standardization related to generative UI.
* Clarify security guardrail specifications.
* Build more universal DSL specifications.
* Research multi-platform unified rendering frameworks.

---

### 3. Ming Zu (Baidu)

Introduced Baidu's current work in the area of generative UI. Overall, the practical path is quite similar to the ideas shared by the previous two speakers.

Since beginning to explore AI applications in early 2023, Baidu has also continuously made various extensions based on Markdown to support richer and more powerful components within conversational product forms. Simultaneously, in the process of building the Agent ecosystem, and from the perspective of reducing developer integration costs, they also defined a JSON DSL to provide UI layout and interaction capabilities. Through constant iteration over the past few years, a relatively mature solution has gradually taken shape.

At the end of last year, they systematically integrated the technical accumulations from the past few years, including core capabilities such as protocols, rendering engines, and component libraries, and open-sourced it externally ðŸ‘‰ [https://github.com/baidu/cosui/](https://github.com/baidu/cosui/)

Recent key research directions involve exploring how to use large models to directly generate code, providing more diverse and rich interactive applications in search scenarios. In this process, they have also encountered some common issues mentioned by the previous speakers. Therefore, they also hope to further exchange ideas with the industry in the future to jointly promote the development and adoption of related technologies.

---
## Open Discussion Session

### Q1: Is loading speed still important for UI interfaces generated by large models in AI chat scenarios?

**Zuo Wang:**
In the development of traditional mobile technologies, we experienced the evolution path of "Native â†’ Web â†’ Cross-platform Technology Stacks". The core goal of cross-platform technology is to maintain the advantage of cross-platform while approximating the performance and loading speed of native applications as much as possible. The reason cross-platform technology solutions emerged is essentially because users were generally dissatisfied with the "slow loading" experience of Web pages.

However, in the current AI conversation and generative application scenarios, user perception has undergone some changes: in traditional page scenarios, users default to expecting "click for immediate response" and are very sensitive to speed; whereas in AI chat scenarios, users are more inclined to view the system as an "intelligent agent with thinking ability", and therefore possess higher tolerance and patience for response speed.

In generative UI scenarios, if the interface content is dynamically loaded and generated through Web technology, does its loading speed still have a critical impact on user experience like in traditional applications? In other words, will users lower their expectations for immediate performance due to the psychological anticipation that "AI needs to think"? I hope to understand various vendors' perspectives and requirement positioning on this issue in their actual product design.

**Ming Zu:**
In Baidu's search scenario, our performance requirements are still relatively high at present, which is also a relatively significant business scenario constraint and challenge we are currently facing.

**Ruoxi Ran:**
I'd like to briefly add some thoughts regarding performance. We will continue to advance related work and discuss further with everyone at an appropriate time.

While listening to Chunhui's presentation earlier, I noticed a point he mentionedâ€”the "impossible triangle" that generative UI currently faces. This also sparked some thinking on my part. Currently, in the process of using large models, whether through API calls, deploying models locally, or using large models through Web applications, people seem to have a relatively high tolerance for the response time of model-generated content.

From the current stage, users generally accept the characteristic that large models "need thinking time" and have relatively lenient real-time requirements. However, from a longer-term perspective, I am not sure if this tolerance will persist. Looking back at the development history of Web technology: in the early stages, users also had high tolerance for webpage loading speeds, even accepting waits of a minute; and with technological advancements and improved experience, user demands for response time have significantly increased now.

Therefore, I am considering whether a similar trend will occur in the process of large models and AI applications gradually becoming widespreadâ€”user expectations for performance will gradually increase, and tolerance for latency will gradually decrease.

At the current technological stage, we may indeed need to face a similar "impossible triangle" problem: how to achieve a balance among performance, flexibility, and experience. How to find a reasonable balance point while continuously optimizing performance, I believe this is a very worthy topic for discussion. Whether within W3C or across the entire industry, it is necessary to have more in-depth discussions around this issue.

**Chunhui Mo:**
I would like to talk about the relationship between generative UI and large models from the perspective of performance metrics.

In the field of large models, there are two very important performance metrics: TTFT (Time To First Token) and TPOT (Time Per Output Token). These two metrics have become key standards for measuring large model performance. I am thinking, for generative UI, should we also establish a similar measurement system to evaluate performance at the user experience level?

Currently, a feasible idea is to refer to the relatively mature indicator system of large models and use them as inputs for designing generative UI metrics. For example, during the large model generation process, even if the final result is not fully output, as long as some feedback can be given to the user earlyâ€”whether it's the display of a chain of thought or the gradual presentation of partial contentâ€”it is itself an effective "response". The key lies in: the waiting time from the user clicking submit to seeing the first effective feedback, and the user's tolerance for this waiting time.

In the era of traditional static UI, we usually followed the "three-second rule", meaning users found it hard to accept no response beyond three seconds, otherwise it would cause significant drop-off. But in generative UI scenarios, this standard is clearly no longer applicable. The computational process of large models inherently requires time, so user psychological expectations and tolerance will also be higher. However, if there is no feedback at all, making users wait for a blank interface for one or two minutes, is equally unacceptable.

Therefore, I believe the next key point for discussion is how should the performance metrics for generative UI be defined?

Can we learn from the TTFT concept of large models and take "the time when the first visible feedback appears" as a core measurement standard?

Looking further, for UI scenarios, we need more granular metrics. For example:
- The time when the first UI element appears;
- The rhythm of subsequent content being rendered progressively;
- The output interval of each text character or each UI component module.

Whether it's the gradual presentation of text content or the progressive loading of various components in a graphical interface, it essentially resembles the streaming output of tokens. We can completely take "the output time of each visual component" as an important dimension for measuring generative UI performance.

In summary, my suggestion is: refer to the TTFT and token output rhythm of large models to establish a set of quantitative performance indicator systems applicable to generative UI, used to evaluate "first screen display time" and "the efficiency of subsequent UI elements being gradually presented". Only in this way can we provide the industry with a unified, quantifiable, and evaluable standard, and also be more conducive to optimizing the real user experience.

---

### Q2: How to optimize platforms and ecosystems through standard collaboration

**Zuo Wang:**
Let me add another thought. The previous speakers all mentioned the issue of "proprietary standards", which is also a point I have been focusing on and thinking about.

From my understanding, whether facing service providers, large model platform vendors, or digital environment vendors, all parties are currently formulating their own UI description specifications based on their own needs. Regarding this phenomenon, I have always held a question:
Is it necessary, and is it possible, for us to promote the unification of standards related to generative UI?

The current trend is that large models are gradually becoming service distribution platforms. When they complete service distribution, they will eventually present an interaction interface to the user, and this interface is often automatically generated by the model. Moreover, in practical scenarios, this process is not only completed by a single model but may also involve collaboration among multiple service providers and multiple AI Agents, ultimately jointly generating a resulting interface.

In this case, if each model and each platform defines its own proprietary UI description method, the threshold may seem low, but it is not friendly to the entire ecosystem in the long run. For example, if there are ten large model vendors in the market, and each adopts a different interface description standard, then as an application developer or service provider, they need to adapt to ten different protocols and parsing methods respectively. This will undoubtedly significantly increase integration costs and is not conducive to the healthy development of the technological ecosystem.

Therefore, the core point I want to express is: at the description level of generative UIâ€”whether based on HTML/CSS or more abstract DSL formsâ€”can we first reach a basic consensus: to promote the standardization and unification of this layer as much as possible?

If a consensus can be formed on this point, allowing different models and platforms to follow relatively consistent description specifications, it will be more beneficial to developers, service providers, and the entire industrial ecosystem, and can also better promote the long-term development of generative UI technology.

**Chunming Hu:**
Let me briefly share two points of view. First, I believe at the layer of final UI presentation, a standard is definitely still needed. Everyone mentioned the concept of "subset" earlier. From the current technological form, no matter what form generative UI outputs, it ultimately requires a runtime environment to render. Even if today's large model only outputs Markdown, it still needs a renderer to parse and display it. If the interaction is more complex, richer API support is needed, and ultimately it still has to be handed over to a runtime layer similar to View to complete display and interaction.

So architecturally, there is naturally a stratification here:
- One layer is the large model generating a certain constrained DSL;
- Another layer is a rendering runtime from lightweight to complex, used to parse this DSL and implement interaction.

Whether we like it or not subjectively, this layered structure of "generation layer + runtime layer" actually exists objectively.

The second point of view is about cross-vendor unification. In theory, each vendor can define a DSL only for its own model, as long as its own runtime can parse it. But if no standardization is done at all, it will lead to a lot of reinventing the wheel: different vendors will have to develop and maintain similar runtimes and parsing mechanisms respectively, which is costly and not conducive to ecosystem development.

If this layer can be standardized to a certain extent, or at least define several "subset specifications" of capability levels, it can decouple content supply and runtime, which is meaningful for the entire industry.

Also, there is a practical requirement: in the future, multiple large models and multiple service parties may jointly participate in generating an interface. For example, a user accesses Ctrip or airline services through a certain model. From the brand perspective, they hope the interface style can conform to their own brand consistency. If a middle-layer mechanism similar to templates can be provided, allowing generative UI to fill content on the basis of established templates, perhaps this demand can be better met.

In summary, if these demands are indeed widespread, then defining some cross-vendor specifications at the middle layer is not just a technical optimization but a direction with practical industrial value. These are some of my observations, for your reference.

---

### Q3: UI Components

**Chunhui Mo:**
Actually, we have discussed this topic before. For generative UI, a very core issue is the convergence of UI components. Since it's UI, it necessarily involves components, and currently, the component libraries used by various vendors when developing applications vary greatly. It's almost impossible to require all vendors to use the same set of component libraries to achieve interface display.

If we hope to build a generative UI rendering engine, letting the content output by large models be rendered through a runtime, we will inevitably encounter a practical problem: what component system should this rendering engine be based on?

Each vendor, each business scenario has its own set of component implementations, making it difficult to forcibly unify at the framework level. Therefore, whether at the framework layer or the component library layer, it's actually difficult to achieve complete convergence. In this case, if we want to discuss a standardized runtime environment that can be widely accepted, I believe the only direction that truly has the possibility of convergence at present is still based on Web technologyâ€”this is also the core domain of W3C.

Based on this idea, we are also exploring internally. The current approach is: allow large models to output DSL describing component usage; support different technology stacks (e.g., React, Vue, Web Components, etc.); before rendering, unify components from different technology stacks into Web standard components for processing.

We have also referred to some existing practices in this regard, such as Google's A2UI protocol. Through research, it can be found that Google is also trying to do similar convergence: its examples support both Angular components and Web Components, and plan to support more frameworks. From this perspective, achieving unification through Web Components at the bottom is a relatively feasible technical path.

Our idea is similar: before passing components to the rendering engine, unify them into Web Components first. Since related technologies are relatively mature, this conversion itself is not difficult. In this way, during actual rendering, the rendering engine only needs to handle standard Web components without needing to care about which specific framework or component library is used at the upper layer, achieving consistent display effects in the browser environment.

However, it should be noted that if the DSL protocol directly includes the usage method of a specific component library, such a protocol itself is difficult to become a universal standard. A more reasonable approach is similar to the A2UI idea: the standard level does not specify the specific format of components, but defines a mechanism that allows developers to declare to the large model in advance the "available component set and its parameter capabilities". The large model only needs to generate UI based on these capabilities without needing to care about the specific implementation details of the components.

I believe that a certain degree of convergence and standardization can be achieved at this level, which may also be a direction we can focus on promoting subsequently.

---

### Q4: Web Subset

**Xiaoqian Wu:**
Introduced the [joint discussion](https://www.w3.org/community/high-perf-baseline/2025/12/17/tpac-2025/) of the W3C MiniApps Working Group, the W3C High-Performance Web Applications Community Group, the WebView Community Group, the IWA Community, and the PWA Widget Community.

**Fuqiao Xue:**
In the discussions on GitHub, I also wrote down some of my thoughts. Combining the sharing from the experts just now, I believe the development of generative UI has two main directions:
- Directly generating Web pages, i.e., generating HTML/CSS/JS code;
- Generating restricted card forms based on component libraries, for example, defining components using JS, XML, etc., and then rendering them through component libraries.

I believe these two directions are not mutually exclusive and can be considered simultaneously.

For the direction of directly generating Web, its flexibility and upper limit are very high, capable of generating rich interfaces, but it also brings some issues, especially security and communication issues. To balance flexibility and security, the scheme I previously considered is:
- Establish a restricted subset of HTML/CSS/JS, accompanied by rendering guidelines;
- Similar to the runtime environment of mini-programs, stipulate allowed elements and attributes while excluding high-risk or high-resource-consumption features, such as prohibiting the use of `<script>` tags and prohibiting loading excessive external resources;
- Ensure security through sandboxing rules.

On the other hand, we can also consider developing standardized DSLs:
- Enable interfaces generated by models or service providers to be reliably rendered in different environments;
- When training new models, the model can learn DSL grammar to produce interfaces more efficiently;
- The specific syntax and design of the DSL need to be continuously explored and adjusted in practice.

Additionally, regarding the component library issue, W3C's [Open UI Community Group](https://www.w3.org/groups/cg/open-ui/) is currently working on UI component standardization. We can pay attention and refer to their ideas.

In general, I believe in the future we can explore both pathsâ€”direct Web and DSL standardizationâ€”simultaneously to provide feasible solutions for the security, controllability, and cross-platform rendering of generative UI.

---

### Q5: Testing and Verification

**Zuo Wang:**
Regarding the testing and verification of generative UI, I have a few thoughts:

In traditional Web development, humans conduct testing before going live, but generative UI is generated by models, especially for complex interfaces, making it impossible to rely entirely on manual verification.

Therefore, it is necessary to establish specialized testing processes or infrastructure, possibly deploying virtual environments on the client side, allowing the interfaces generated by models to be automatically verified for correctness and reliability in test environments first.

Practical cases show that some companies automatically generate test cases using models and run them in server-side or virtual environments to verify the correctness of generated outputs before delivering them to users.

In summary, the reliability verification of generative UI should rely more on automated testing and virtualization environments rather than traditional manual testing. This is an important means to ensure the reliability of the content seen by users.

**Ruoxi Ran:**
Regarding the accessibility and testing of generative UI, I have the following thoughts:
- UI is highly related to accessibility. If the content of generative UI does not comply with accessibility standards, it may bring compliance risks, such as facing product removal in Europe and the US, etc.;
- Some existing automated accessibility testing tools still cannot cover all standards, so there remains a gap in accessibility verification for generative UI;
- W3C can learn from the experience of the WPT ([Web Platform Tests](https://github.com/web-platform-tests/wpt)) platform and consider developing an open-source testing platform or tool for automated verification of generative UI, including accessibility testing;
- The goal is to establish a set of shareable open-source methods and platforms, enabling different vendors and developers to uniformly test the accessibility and other key quality metrics of generative UI.

**Fuqiao Xue:**
Regarding the automated testing and verification of generative UI, I believe the following directions can be considered:
- W3C currently has the Browser Testing and Tools Working Group, which includes the WebDriver protocol and can serve as a reference or foundation;
- Methods for automatically generating, running, and evaluating generated UI can be explored, for example:
  - Automatically taking screenshots or capturing the generated UI;
  - Comparing the generated UI with user intent for verification;
  - If verification fails, the system can automatically attempt repairs until it meets the verification standards.

These methods can provide quantifiable testing means for the reliability and user experience of generative UI and can also serve as the basis for subsequent standardization and tooling.

---

## Conclusions & Next Steps

The Generative UI Community Group was established following this meeting and plans to conduct further exploration around the following areas:

- **Evaluation & Performance:** Explore how to define and evaluate the latency, responsiveness, and output quality of Generative UI in Web environments, and identify gaps in existing W3C standardization work.
- **Verification & Testing:** Investigate verification and testing methods for Generative UI. These methods should go beyond traditional end-to-end testing and can include the application of structured test cases and reference implementations.
- **Intermediate Representation:** Explore the potential value of lightweight, cross-vendor intermediate representations or protocols for Generative UI, with the aim of improving its interoperability and model compatibility.
- **Alignment with Web Platform:** Examine whether Generative UI systems could benefit from a lightweight subset of Web technologies, and the need to remain consistent with existing Web principles.

Welcome to join the [Generative UI Community Group](https://www.w3.org/groups/cg/gen-ui/) and participate in related discussions.
